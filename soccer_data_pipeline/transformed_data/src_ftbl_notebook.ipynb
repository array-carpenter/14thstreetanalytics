{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thank you for your application for the Data Engineer role at src | ftbl.  We are overwhelmed with interest and are happy to share that you've been selected to participate in the next round. The hiring process will consist of 3 parts: \n",
    "\n",
    "# 1.     a take home project, \n",
    "\n",
    "# 2.     a technical interview and \n",
    "\n",
    "# 3.     a general interview.\n",
    "\n",
    "# After we have interviewed all the candidates, we hope to make a decision as quickly as possible. We will try to keep candidates informed of the timelines as much as possible.\n",
    "\n",
    "\n",
    "# For the take home project, we would like you to use the attached SkillCorner data to complete the following tasks:\n",
    "\n",
    "# Task 1\n",
    "\n",
    "# Overview:\n",
    "\n",
    "# Write an ETL process to store the data in formats to be used by the data science team.  It is up to you to determine what the format and schema should be based on what you anticipate the questions of the data science team will be.\n",
    "\n",
    "# Deliverables:\n",
    "\n",
    "# · Source code for the ETL process\n",
    "\n",
    "# · Sample output (please do not send back the full data set)\n",
    "\n",
    "# · A write up explaining your thought process as well as answers to the following questions:\n",
    "\n",
    "# Why did you choose this schema?\n",
    "# What alternatives did you consider?\n",
    "# o   What are the trade offs in terms of running time, compute cost and storage cost?\n",
    "\n",
    "# o   Are there any concerns with scale?\n",
    "\n",
    "# o   What might change if you were asked to merge this with event data (for example Statsbomb or WyScout)\n",
    "\n",
    " \n",
    "# Task 2\n",
    "\n",
    "# Overview:\n",
    "\n",
    "# Using the transformed data from the first task, write a script in PySpark or an equivalent language, that determines for each player the most “intense” 5 minute segment of the match.  Additionally, calculate a metric called “spread” which is the distance between all players on the field at any moment in time and find the 2 minute segment with the highest spread. How do these two metrics relate to each other?\n",
    "\n",
    " \n",
    "\n",
    "# Deliverables:\n",
    "\n",
    "# · Source code for finding the most intense 5 minutes segment\n",
    "\n",
    "# · Source code for calculating the spread\n",
    "\n",
    "# · A small write up of how you defined and found the most intense 5 minute segment, the 2 minute segment with the highest spread, and a brief description of how these two metrics are related to each other\n",
    "\n",
    " \n",
    "\n",
    "# What are we looking for?\n",
    "\n",
    "# We are not looking for perfect or “right” answers - these are difficult questions and there are many unique ways to answer them.  We want to see how you interpret and approach the problems both from a technical and football perspective as well as how you communicate your thought process.\n",
    "\n",
    " \n",
    "\n",
    "# Submissions\n",
    "\n",
    "# Please submit your project as a single zip file to info@srcftbl.com along with the following information:\n",
    "\n",
    "# 1.     Are you eligible to work in your current country of residence? If not, please explain your situation and desired resolution\n",
    "\n",
    "# 2.     General availability for follow up interviews over the next three weeks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder successfully unzipped to: C:/Users/RaymondCarpenter/Documents/GitHub/14thstreetanalytics/src_ftbl/src_de_sample_data\n"
     ]
    }
   ],
   "source": [
    "# Write an ETL process to store the data in formats to be used by the data science team.  It is up to you to determine what the format and schema should be based on what you anticipate the questions of the data science team will be.\n",
    "\n",
    "# Deliverables:\n",
    "\n",
    "# · Source code for the ETL process\n",
    "\n",
    "# · Sample output (please do not send back the full data set)\n",
    "\n",
    "# · A write up explaining your thought process as well as answers to the following questions:\n",
    "\n",
    "# Why did you choose this schema?\n",
    "# What alternatives did you consider?\n",
    "# o   What are the trade offs in terms of running time, compute cost and storage cost?\n",
    "\n",
    "# o   Are there any concerns with scale?\n",
    "\n",
    "# o   What might change if you were asked to merge this with event data (for example Statsbomb or WyScout)\n",
    "\n",
    "# Extract - Unzip Folder \n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "def unzip_folder(zip_file_path, extract_to):\n",
    "    if not zipfile.is_zipfile(zip_file_path):\n",
    "        print(\"Invalid zip file.\")\n",
    "        return\n",
    "    if not os.path.exists(extract_to):\n",
    "        os.makedirs(extract_to)\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "        # Extract all contents to the specified directory\n",
    "        zip_ref.extractall(extract_to)\n",
    "        print(\"Folder successfully unzipped to:\", extract_to)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage:\n",
    "    zip_file_path = \"C:/Users/RaymondCarpenter/Documents/GitHub/14thstreetanalytics/src_ftbl/src_de_sample_data.zip\" \n",
    "    extract_to = \"C:/Users/RaymondCarpenter/Documents/GitHub/14thstreetanalytics/src_ftbl/src_de_sample_data\"\n",
    "\n",
    "    unzip_folder(zip_file_path, extract_to)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All CSV files have been combined into: combined_metadata.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\RaymondCarpenter\\AppData\\Local\\Temp\\ipykernel_28852\\1636979352.py:32: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  all_data = all_data.append(flat_data, ignore_index=True)\n",
      "C:\\Users\\RaymondCarpenter\\AppData\\Local\\Temp\\ipykernel_28852\\1636979352.py:32: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  all_data = all_data.append(flat_data, ignore_index=True)\n",
      "C:\\Users\\RaymondCarpenter\\AppData\\Local\\Temp\\ipykernel_28852\\1636979352.py:32: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  all_data = all_data.append(flat_data, ignore_index=True)\n",
      "C:\\Users\\RaymondCarpenter\\AppData\\Local\\Temp\\ipykernel_28852\\1636979352.py:32: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  all_data = all_data.append(flat_data, ignore_index=True)\n",
      "C:\\Users\\RaymondCarpenter\\AppData\\Local\\Temp\\ipykernel_28852\\1636979352.py:32: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  all_data = all_data.append(flat_data, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "# List of JSON file paths\n",
    "json_files = ['C:/Users/RaymondCarpenter/Documents/GitHub/14thstreetanalytics/src_ftbl/src_de_sample_data/10000_metadata.json', 'C:/Users/RaymondCarpenter/Documents/GitHub/14thstreetanalytics/src_ftbl/src_de_sample_data/10017_metadata.json', \n",
    "              'C:/Users/RaymondCarpenter/Documents/GitHub/14thstreetanalytics/src_ftbl/src_de_sample_data/10013_metadata.json', \n",
    "              'C:/Users/RaymondCarpenter/Documents/GitHub/14thstreetanalytics/src_ftbl/src_de_sample_data/100094_metadata.json',\n",
    "              'C:/Users/RaymondCarpenter/Documents/GitHub/14thstreetanalytics/src_ftbl/src_de_sample_data/10009_metadata.json']\n",
    "\n",
    "\n",
    "# Initialize an empty DataFrame to store all data\n",
    "all_data = pd.DataFrame()\n",
    "\n",
    "# Process each JSON file\n",
    "for json_file in json_files:\n",
    "    # Read JSON data from the file\n",
    "    with open(json_file, 'r') as file:\n",
    "        json_data = json.load(file)\n",
    "\n",
    "    # Flatten the nested structure of the main JSON\n",
    "    flat_data = pd.json_normalize(json_data)\n",
    "\n",
    "    # Unroll the 'players' column\n",
    "    players_data = pd.json_normalize(json_data['players'])\n",
    "    flat_data = pd.concat([flat_data, players_data], axis=1).drop('players', axis=1)\n",
    "\n",
    "    # Add a new column for the 'id' from the JSON file\n",
    "    flat_data['game_id'] = json_data['id']\n",
    "\n",
    "    # Append the data to the all_data DataFrame\n",
    "    all_data = all_data.append(flat_data, ignore_index=True)\n",
    "\n",
    "# Save the combined data to a CSV file\n",
    "all_csv_path = 'combined_metadata.csv'\n",
    "all_data.to_csv(all_csv_path, index=False)\n",
    "\n",
    "print(f'All CSV files have been combined into: {all_csv_path}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file created successfully at: output_metadata.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "\n",
    "# Open the input file\n",
    "input_file_path = '10000_tracking.txt'\n",
    "with open(input_file_path, 'r') as input_file:\n",
    "    # Read lines from the file\n",
    "    lines = input_file.readlines()\n",
    "\n",
    "# Output CSV file path\n",
    "output_csv_path = 'output.csv'\n",
    "\n",
    "# Open the CSV file for writing\n",
    "with open(output_csv_path, 'w', newline='') as output_csv:\n",
    "    # Create a CSV writer object\n",
    "    csv_writer = csv.writer(output_csv)\n",
    "\n",
    "    # Write header to CSV\n",
    "    csv_writer.writerow([\"track_id\", \"trackable_object\", \"is_visible\", \"x\", \"y\", \"z\", \"frame\", \"timestamp\"])\n",
    "\n",
    "    # Iterate through each line in the input file\n",
    "    for line in lines:\n",
    "        # Load JSON data from the line\n",
    "        data = json.loads(line)\n",
    "\n",
    "        # Extract relevant information\n",
    "        frame = data.get(\"frame\")\n",
    "        timestamp = data.get(\"timestamp\")\n",
    "\n",
    "        for item in data.get(\"data\", []):\n",
    "            track_id = item.get(\"track_id\")\n",
    "            trackable_object = item.get(\"trackable_object\")\n",
    "            is_visible = item.get(\"is_visible\")\n",
    "            x = item.get(\"x\", 0.0)\n",
    "            y = item.get(\"y\", 0.0)\n",
    "            z = item.get(\"z\", 0.0)\n",
    "\n",
    "            # Write the extracted data to CSV\n",
    "            csv_writer.writerow([track_id, trackable_object, is_visible, x, y, z, frame, timestamp])\n",
    "\n",
    "print(f'CSV file created successfully at: {output_csv_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "# Example CSV data\n",
    "\n",
    "# Read CSV data into a DataFrame\n",
    "df = pd.read_csv('C:/Users/RaymondCarpenter/Documents/GitHub/14thstreetanalytics/src_ftbl/combined_metadata.csv')\n",
    "\n",
    "df.head()\n",
    "# Define schemas based on related columns\n",
    "DimStadium = df[['stadium.id', 'stadium.name', 'stadium.city', 'stadium.capacity','pitch_length','pitch_width']]\n",
    "DimTeam = df[['home_team.id', 'home_team.name', 'home_team.short_name', 'home_team.acronym',\n",
    "                   'away_team.id', 'away_team.name', 'away_team.short_name', 'away_team.acronym']]\n",
    "DimKit = df[['home_team_kit.id', 'home_team_kit.team_id', 'home_team_kit.season.id',\n",
    "                  'home_team_kit.season.start_year', 'home_team_kit.season.end_year',\n",
    "                  'home_team_kit.season.name', 'home_team_kit.name', 'home_team_kit.jersey_color',\n",
    "                  'home_team_kit.number_color', 'away_team_kit.id', 'away_team_kit.team_id',\n",
    "                  'away_team_kit.season.id', 'away_team_kit.season.start_year',\n",
    "                  'away_team_kit.season.end_year', 'away_team_kit.season.name',\n",
    "                  'away_team_kit.name', 'away_team_kit.jersey_color', 'away_team_kit.number_color']]\n",
    "DimSeason = df[['competition_edition.season.id','competition_edition.season.start_year','competition_edition.season.end_year']]\n",
    "DimCoach = df[['home_team_coach.id', 'home_team_coach.first_name', 'home_team_coach.last_name',\n",
    "                    'away_team_coach.id', 'away_team_coach.first_name', 'away_team_coach.last_name']]\n",
    "DimCompetition = df[['competition_edition.id', 'competition_edition.competition.id',\n",
    "                          'competition_edition.competition.area', 'competition_edition.competition.name',\n",
    "                          'competition_edition.season.id', 'competition_edition.season.start_year',\n",
    "                          'competition_edition.season.end_year', 'competition_edition.season.name',\n",
    "                          'competition_edition.name', 'competition_round.id', 'competition_round.name',\n",
    "                          'competition_round.round_number', 'competition_round.potential_overtime']]\n",
    "DimCompetitionRound = df[['competition_round.id','competition_edition.name','competition_round.round_number','competition_round.potential_overtime']]\n",
    "DimCompetionEdition = df[['competition_edition.id','competition_edition.competition.id','competition_edition.competition.area','competition_edition.competition.name','competition_edition.season.id']]\n",
    "DimPlayer = df[['id.1', 'first_name', 'last_name', 'short_name', 'birthday', 'trackable_object',\n",
    "                     'gender', 'player_role.id', 'player_role.name', 'player_role.acronym']]\n",
    "DimPlayerRole = df[['player_role.id','player_role.name','player_role.acronym']]\n",
    "FactMatch = df[['id', 'home_team_score','away_team_score','date_time','referees','status','home_team_side','stadium.id','home_team.id','away_team.id','competition_edition.season.id','home_team_coach.id','away_team_coach.id','competition_edition.id',\n",
    "                'competition_edition.competition.id','competition_round.id','start_time','end_time','away_team_kit.id','ball.trackable_object',\n",
    "                'yellow_card','red_card','goal','own_goal','injured']]\n",
    "\n",
    "# # Save each schema to a separate CSV file\n",
    "DimStadium.to_csv('DimStadium.csv', index=False)\n",
    "DimTeam.to_csv('DimTeam.csv', index=False)\n",
    "DimKit.to_csv('DimKit.csv', index=False)\n",
    "DimCoach.to_csv('DimCoach.csv', index=False)\n",
    "DimCompetition.to_csv('DimCompetition.csv', index=False)\n",
    "DimPlayer.to_csv('DimPlayer.csv', index=False)\n",
    "DimCompetionEdition.to_csv('DimCompetitionEdition.csv',index=False)\n",
    "DimCompetitionRound.to_csv('DimCompetitionRound.csv',index=False)\n",
    "DimSeason.to_csv('DimSeason.csv',index=False)\n",
    "DimPlayerRole.to_csv('DimPlayerRole.csv',index=False)\n",
    "FactMatch.to_csv('FactMatch.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file created successfully at: DimTracking.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "# List of input files\n",
    "input_files = ['C:/Users/RaymondCarpenter/Documents/GitHub/14thstreetanalytics/src_ftbl/src_de_sample_data/10000_tracking.txt', \n",
    "               'C:/Users/RaymondCarpenter/Documents/GitHub/14thstreetanalytics/src_ftbl/src_de_sample_data/10009_tracking.txt', \n",
    "               'C:/Users/RaymondCarpenter/Documents/GitHub/14thstreetanalytics/src_ftbl/src_de_sample_data/10013_tracking.txt', \n",
    "               'C:/Users/RaymondCarpenter/Documents/GitHub/14thstreetanalytics/src_ftbl/src_de_sample_data/100094_tracking.txt', \n",
    "               'C:/Users/RaymondCarpenter/Documents/GitHub/14thstreetanalytics/src_ftbl/src_de_sample_data/10017_tracking.txt']\n",
    "\n",
    "# Output CSV file path\n",
    "output_csv_path = 'DimTracking.csv'\n",
    "\n",
    "# Open the CSV file for writing\n",
    "with open(output_csv_path, 'w', newline='') as output_csv:\n",
    "    # Create a CSV writer object\n",
    "    csv_writer = csv.writer(output_csv)\n",
    "\n",
    "    # Write header to CSV\n",
    "    csv_writer.writerow([\"game_id\", \"track_id\", \"trackable_object\", \"is_visible\", \"x\", \"y\", \"z\", \"frame\", \"timestamp\"])\n",
    "\n",
    "    # Iterate through each input file\n",
    "    for input_file_path in input_files:\n",
    "        # Extract game_id using regular expression\n",
    "        match = re.search(r'(\\d+)_tracking\\.txt', os.path.basename(input_file_path))\n",
    "        game_id = match.group(1) if match else None\n",
    "\n",
    "        # Open the input file\n",
    "        with open(input_file_path, 'r') as input_file:\n",
    "            # Read lines from the file\n",
    "            lines = input_file.readlines()\n",
    "\n",
    "        # Iterate through each line in the input file\n",
    "        for line in lines:\n",
    "            # Load JSON data from the line\n",
    "            data = json.loads(line)\n",
    "\n",
    "            # Extract relevant information\n",
    "            frame = data.get(\"frame\")\n",
    "            timestamp = data.get(\"timestamp\")\n",
    "\n",
    "            for item in data.get(\"data\", []):\n",
    "                track_id = item.get(\"track_id\")\n",
    "                trackable_object = item.get(\"trackable_object\")\n",
    "                is_visible = item.get(\"is_visible\")\n",
    "                x = item.get(\"x\", 0.0)\n",
    "                y = item.get(\"y\", 0.0)\n",
    "                z = item.get(\"z\", 0.0)\n",
    "\n",
    "                # Write the extracted data to CSV\n",
    "                csv_writer.writerow([game_id, track_id, trackable_object, is_visible, x, y, z, frame, timestamp])\n",
    "\n",
    "print(f'CSV file created successfully at: {output_csv_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming your CSV file is named \"your_file.csv\"\n",
    "file_path = \"C:/Users/RaymondCarpenter/Documents/GitHub/14thstreetanalytics/src_ftbl/output_tracking.csv\"\n",
    "\n",
    "# Read the CSV file into a pandas DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Filter the DataFrame based on the condition\n",
    "filtered_df = df[df['trackable_object'] == 18731]\n",
    "\n",
    "# Display the filtered DataFrame\n",
    "filtered_df.to_csv('mcgoldrick_tracking.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------+--------+----------+-----------------+-----------------+\n",
      "|trackable_object|game_id|track_id|is_visible|        intensity|        intensity|\n",
      "+----------------+-------+--------+----------+-----------------+-----------------+\n",
      "|           18731|  10000|   18731|      true|673305.5596246592|673305.5596246592|\n",
      "+----------------+-------+--------+----------+-----------------+-----------------+\n",
      "\n",
      "+-------+-------+--------+----------------+----------+-----+-----+---+-----+---------+-----------------+--------------------+------------------+\n",
      "|    _c0|game_id|track_id|trackable_object|is_visible|    x|    y|  z|frame|timestamp|         distance|              spread|       spread_2min|\n",
      "+-------+-------+--------+----------------+----------+-----+-----+---+-----+---------+-----------------+--------------------+------------------+\n",
      "|1051026|  10000|   18731|           18731|      true|10.85|-20.2|0.0|57747| 01:33:07|2252.745064556146|1.0230466380205823E7|271897.55060692166|\n",
      "+-------+-------+--------+----------------+----------+-----+-----+---+-----+---------+-----------------+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lag, sqrt, sum,when\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"intense_spread_analysis\").getOrCreate()\n",
    "\n",
    "df = spark.read.csv(\"C:/Users/RaymondCarpenter/Documents/GitHub/14thstreetanalytics/src_ftbl/mcgoldrick_tracking.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Define a window specification based on timestamp\n",
    "time_window_spec = Window().orderBy(\"timestamp\")\n",
    "\n",
    "# Calculate distance between players using Euclidean distance\n",
    "def calculate_distance(x1, y1, z1, x2, y2, z2):\n",
    "    return sqrt((x1 - x2)**2 + (y1 - y2)**2 + (z1 - z2)**2)\n",
    "\n",
    "# Calculate distance between all players for each frame\n",
    "df = df.withColumn(\"distance\", \n",
    "                  sum(when(\n",
    "                      lag(\"x\").over(time_window_spec).isNotNull(),\n",
    "                      calculate_distance(\n",
    "                          col(\"x\"), col(\"y\"), col(\"z\"),\n",
    "                          lag(\"x\").over(time_window_spec),\n",
    "                          lag(\"y\").over(time_window_spec),\n",
    "                          lag(\"z\").over(time_window_spec)\n",
    "                      )\n",
    "                  ).otherwise(0)).over(time_window_spec))\n",
    "\n",
    "\n",
    "# Define a window specification based on frame\n",
    "frame_window_spec = Window().orderBy(\"frame\").rangeBetween(-5 * 60, 0)\n",
    "\n",
    "# Calculate intensity as the sum of distances for each player over a 5-minute window\n",
    "df_intensity = df.withColumn(\"intensity\", sum(\"distance\").over(frame_window_spec))\n",
    "\n",
    "# Find the most intense 5-minute segment for each player\n",
    "df_most_intense = df_intensity.groupBy(\n",
    "    \"trackable_object\", \"game_id\", \"track_id\", \"is_visible\", \"intensity\"\n",
    ").agg(\n",
    "    col(\"intensity\")\n",
    ").orderBy(col(\"intensity\").desc()).limit(1)\n",
    "\n",
    "# Calculate spread for each frame\n",
    "df_spread = df.withColumn(\"spread\", sum(\"distance\").over(time_window_spec))\n",
    "\n",
    "# Define a window specification based on frame for the 2-minute segment\n",
    "frame_window_spec_2min = Window().orderBy(\"frame\").rangeBetween(-2 * 60, 0)\n",
    "\n",
    "# Find the 2-minute segment with the highest spread\n",
    "df_highest_spread = df_spread.withColumn(\"spread_2min\", sum(\"distance\").over(frame_window_spec_2min))\n",
    "\n",
    "# Show the results\n",
    "df_most_intense.show()\n",
    "df_highest_spread.orderBy(col(\"spread_2min\").desc()).limit(1).show()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most intense 5-minute window:\n",
      "Window: Row(start=datetime.datetime(2023, 12, 8, 1, 25), end=datetime.datetime(2023, 12, 8, 1, 30))\n",
      "Intensity: 3149.350000000028\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, window, sum\n",
    "from pyspark.sql.types import FloatType, TimestampType\n",
    "\n",
    "# Step 1: Create SparkSession\n",
    "spark = SparkSession.builder.appName(\"intense_window\").getOrCreate()\n",
    "\n",
    "# Step 2: Load DataFrame\n",
    "# Assuming your DataFrame is named 'df'\n",
    "# If not, replace 'df' with the actual name of your DataFrame\n",
    "df = spark.read.csv(\"C:/Users/RaymondCarpenter/Documents/GitHub/14thstreetanalytics/src_ftbl/mcgoldrick_tracking.csv\", header=True, inferSchema=True)\n",
    "# Step 3: Convert timestamp to seconds and cast to TimestampType\n",
    "df = df.withColumn(\"timestamp_seconds\", col(\"timestamp\").cast(FloatType()))\n",
    "df = df.withColumn(\"timestamp\", col(\"timestamp\").cast(TimestampType()))\n",
    "\n",
    "# Step 4: Aggregate into 5-minute windows, calculating intensity as the sum of distance\n",
    "intensity_df = df.groupBy(window(\"timestamp\", \"5 minutes\")).agg(\n",
    "    sum(\"x\").alias(\"total_x\"),\n",
    "    sum(\"y\").alias(\"total_y\"),\n",
    "    sum(\"z\").alias(\"total_z\")\n",
    ")\n",
    "\n",
    "# Calculate overall intensity (you may need to customize this based on your specific metrics)\n",
    "intensity_df = intensity_df.withColumn(\"intensity\", col(\"total_x\") + col(\"total_y\") + col(\"total_z\"))\n",
    "\n",
    "# Step 5: Find the 5-minute window with maximum intensity\n",
    "max_intensity_window = intensity_df.orderBy(col(\"intensity\").desc()).first()\n",
    "\n",
    "# Display the result\n",
    "print(\"Most intense 5-minute window:\")\n",
    "print(\"Window:\", max_intensity_window[\"window\"])\n",
    "print(\"Intensity:\", max_intensity_window[\"intensity\"])\n",
    "\n",
    "# Stop SparkSession\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most intense 5-minute window:\n",
      "Window: From 17 to 18 minutes\n",
      "Intensity: 3149.350000000028\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, window, sum\n",
    "from pyspark.sql.types import FloatType, TimestampType\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Step 1: Create SparkSession\n",
    "spark = SparkSession.builder.appName(\"intense_window\").getOrCreate()\n",
    "\n",
    "# Step 2: Load DataFrame\n",
    "# Assuming your DataFrame is named 'df'\n",
    "# If not, replace 'df' with the actual name of your DataFrame\n",
    "df = spark.read.csv(\"C:/Users/RaymondCarpenter/Documents/GitHub/14thstreetanalytics/src_ftbl/mcgoldrick_tracking.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Step 3: Convert timestamp to seconds and cast to TimestampType\n",
    "df = df.withColumn(\"timestamp_seconds\", col(\"timestamp\").cast(FloatType()))\n",
    "df = df.withColumn(\"timestamp\", col(\"timestamp\").cast(TimestampType()))\n",
    "\n",
    "# Step 4: Aggregate into 5-minute windows, calculating intensity as the sum of distance\n",
    "intensity_df = df.groupBy(window(\"timestamp\", \"5 minutes\")).agg(\n",
    "    sum(\"x\").alias(\"total_x\"),\n",
    "    sum(\"y\").alias(\"total_y\"),\n",
    "    sum(\"z\").alias(\"total_z\")\n",
    ")\n",
    "\n",
    "# Calculate overall intensity (you may need to customize this based on your specific metrics)\n",
    "intensity_df = intensity_df.withColumn(\"intensity\", col(\"total_x\") + col(\"total_y\") + col(\"total_z\"))\n",
    "\n",
    "# Step 5: Find the 5-minute window with maximum intensity\n",
    "max_intensity_window = intensity_df.orderBy(col(\"intensity\").desc()).first()\n",
    "\n",
    "# Convert the output to soccer game minute terms (assuming game duration is 92 minutes)\n",
    "game_duration = 92\n",
    "start_minute = int((max_intensity_window[\"window\"][\"start\"].minute + max_intensity_window[\"window\"][\"start\"].hour * 60) / 5)\n",
    "end_minute = int((max_intensity_window[\"window\"][\"end\"].minute + max_intensity_window[\"window\"][\"end\"].hour * 60) / 5)\n",
    "\n",
    "# Display the result\n",
    "print(\"Most intense 5-minute window:\")\n",
    "print(\"Window: From {} to {} minutes\".format(start_minute, end_minute))\n",
    "print(\"Intensity:\", max_intensity_window[\"intensity\"])\n",
    "\n",
    "# Stop SparkSession\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most intense 5-minute window:\n",
      "Window: From 85 to 90 minutes\n",
      "Intensity: 3149.350000000028\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, window, sum\n",
    "from pyspark.sql.types import FloatType, TimestampType\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Step 1: Create SparkSession\n",
    "spark = SparkSession.builder.appName(\"intense_window\").getOrCreate()\n",
    "\n",
    "# Step 2: Load DataFrame\n",
    "# Assuming your DataFrame is named 'df'\n",
    "# If not, replace 'df' with the actual name of your DataFrame\n",
    "df = spark.read.csv(\"C:/Users/RaymondCarpenter/Documents/GitHub/14thstreetanalytics/src_ftbl/mcgoldrick_tracking.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Step 3: Convert timestamp to seconds and cast to TimestampType\n",
    "df = df.withColumn(\"timestamp_seconds\", col(\"timestamp\").cast(FloatType()))\n",
    "df = df.withColumn(\"timestamp\", col(\"timestamp\").cast(TimestampType()))\n",
    "\n",
    "# Step 4: Aggregate into 5-minute windows, calculating intensity as the sum of distance\n",
    "intensity_df = df.groupBy(window(\"timestamp\", \"5 minutes\")).agg(\n",
    "    sum(\"x\").alias(\"total_x\"),\n",
    "    sum(\"y\").alias(\"total_y\"),\n",
    "    sum(\"z\").alias(\"total_z\")\n",
    ")\n",
    "\n",
    "# Calculate overall intensity (you may need to customize this based on your specific metrics)\n",
    "intensity_df = intensity_df.withColumn(\"intensity\", col(\"total_x\") + col(\"total_y\") + col(\"total_z\"))\n",
    "\n",
    "# Step 5: Find the 5-minute window with maximum intensity\n",
    "max_intensity_window = intensity_df.orderBy(col(\"intensity\").desc()).first()\n",
    "\n",
    "# Convert the output to soccer game minute terms (assuming game duration is 92 minutes)\n",
    "game_duration = 92\n",
    "start_minute = int((max_intensity_window[\"window\"][\"start\"].minute + max_intensity_window[\"window\"][\"start\"].hour * 60) / 5) * 5\n",
    "end_minute = start_minute + 5\n",
    "\n",
    "# Display the result\n",
    "print(\"Most intense 5-minute window:\")\n",
    "print(\"Window: From {} to {} minutes\".format(start_minute, end_minute))\n",
    "print(\"Intensity:\", max_intensity_window[\"intensity\"])\n",
    "\n",
    "# Stop SparkSession\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most intense 5-minute window for each game:\n",
      "+-------+------------------+--------------------------+------------------------+\n",
      "|game_id|max_intensity     |max_intensity_window_start|max_intensity_window_end|\n",
      "+-------+------------------+--------------------------+------------------------+\n",
      "|10009  |1007035.169999998 |2023-12-08 00:05:00       |2023-12-08 00:10:00     |\n",
      "|10000  |734362.7400000067 |2023-12-08 00:25:00       |2023-12-08 00:30:00     |\n",
      "|10013  |691442.9399999985 |2023-12-08 00:20:00       |2023-12-08 00:25:00     |\n",
      "|10017  |923188.0400000022 |2023-12-08 00:20:00       |2023-12-08 00:25:00     |\n",
      "|100094 |1262382.6200000085|2023-12-08 00:05:00       |2023-12-08 00:10:00     |\n",
      "+-------+------------------+--------------------------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, window, sum, max, first\n",
    "from pyspark.sql.types import FloatType, TimestampType\n",
    "\n",
    "# Step 1: Create SparkSession\n",
    "spark = SparkSession.builder.appName(\"intense_window\").getOrCreate()\n",
    "\n",
    "# Step 2: Load DataFrame from CSV\n",
    "# Replace 'your_file_path.csv' with the actual path to your CSV file\n",
    "df = spark.read.csv(\"C:/Users/RaymondCarpenter/Documents/GitHub/14thstreetanalytics/src_ftbl/output_tracking.csv\", header=True)\n",
    "\n",
    "# Step 3: Convert timestamp to seconds and cast to TimestampType\n",
    "df = df.withColumn(\"timestamp_seconds\", col(\"timestamp\").cast(FloatType()))\n",
    "df = df.withColumn(\"timestamp\", col(\"timestamp\").cast(TimestampType()))\n",
    "\n",
    "# Step 4: Aggregate into 5-minute windows, calculating intensity as the sum of distance\n",
    "intensity_df = df.groupBy(\"game_id\", window(\"timestamp\", \"5 minutes\")).agg(\n",
    "    sum(\"x\").alias(\"total_x\"),\n",
    "    sum(\"y\").alias(\"total_y\"),\n",
    "    sum(\"z\").alias(\"total_z\"),\n",
    "    window(\"timestamp\", \"5 minutes\").start.alias(\"window_start\"),\n",
    "    window(\"timestamp\", \"5 minutes\").end.alias(\"window_end\")\n",
    ")\n",
    "\n",
    "# Calculate overall intensity (you may need to customize this based on your specific metrics)\n",
    "intensity_df = intensity_df.withColumn(\"intensity\", col(\"total_x\") + col(\"total_y\") + col(\"total_z\"))\n",
    "\n",
    "# Step 5: Find the 5-minute window with maximum intensity for each game\n",
    "max_intensity_per_game = intensity_df.groupBy(\"game_id\").agg(\n",
    "    max(\"intensity\").alias(\"max_intensity\"),\n",
    "    first(\"window_start\").alias(\"max_intensity_window_start\"),\n",
    "    first(\"window_end\").alias(\"max_intensity_window_end\")\n",
    ")\n",
    "\n",
    "# Display the result\n",
    "print(\"Most intense 5-minute window for each game:\")\n",
    "max_intensity_per_game.show(truncate=False)\n",
    "\n",
    "# Stop SparkSession\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most intense 5-minute window for each player, for each game:\n",
      "+-------+----------------+------------------+--------------------------+------------------------+\n",
      "|game_id|short_name      |max_intensity     |max_intensity_window_start|max_intensity_window_end|\n",
      "+-------+----------------+------------------+--------------------------+------------------------+\n",
      "|10000  |G. Wijnaldum    |37136.00999999992 |2023-12-08 00:40:00       |2023-12-08 00:45:00     |\n",
      "|10013  |A. Saint-Maximin|19391.159999999967|2023-12-08 00:35:00       |2023-12-08 00:40:00     |\n",
      "|10017  |P. Aubameyang   |79886.51000000002 |2023-12-08 00:05:00       |2023-12-08 00:10:00     |\n",
      "|10017  |J. Maddison     |29165.170000000027|2023-12-08 00:40:00       |2023-12-08 00:45:00     |\n",
      "|100094 |G. Hanley       |98093.33999999982 |2023-12-08 00:05:00       |2023-12-08 00:10:00     |\n",
      "|100094 |A. Broja        |87644.4399999998  |2023-12-08 00:10:00       |2023-12-08 00:15:00     |\n",
      "|10000  |G. Baldock      |72300.73000000001 |2023-12-08 00:45:00       |2023-12-08 00:50:00     |\n",
      "|10009  |N. Redmond      |65884.73          |2023-12-08 00:20:00       |2023-12-08 00:25:00     |\n",
      "|100094 |M. Aarons       |113183.78         |2023-12-08 00:05:00       |2023-12-08 00:10:00     |\n",
      "|10013  |Rúben Neves     |32274.95999999999 |2023-12-08 00:20:00       |2023-12-08 00:25:00     |\n",
      "|10017  |H. Barnes       |65616.21000000005 |2023-12-08 00:05:00       |2023-12-08 00:10:00     |\n",
      "|10009  |A. Doucouré     |34679.42999999996 |2023-12-08 00:00:00       |2023-12-08 00:05:00     |\n",
      "|10009  |L. Digne        |85456.52000000008 |2023-12-08 00:00:00       |2023-12-08 00:05:00     |\n",
      "|10013  |C. Coady        |49809.42000000005 |2023-12-08 00:00:00       |2023-12-08 00:05:00     |\n",
      "|100094 |Oriol Romeu     |115904.3999999999 |2023-12-08 00:00:00       |2023-12-08 00:05:00     |\n",
      "|100094 |F. Forster      |176982.4000000004 |2023-12-08 00:00:00       |2023-12-08 00:05:00     |\n",
      "|10017  |G. Xhaka        |47397.23000000006 |2023-12-08 00:10:00       |2023-12-08 00:15:00     |\n",
      "|10000  |J. Milner       |8871.810000000003 |2023-12-08 01:25:00       |2023-12-08 01:30:00     |\n",
      "|100094 |J. Ward-Prowse  |116056.0200000002 |2023-12-08 00:10:00       |2023-12-08 00:15:00     |\n",
      "|100094 |S. Armstrong    |103025.57999999986|2023-12-08 00:00:00       |2023-12-08 00:05:00     |\n",
      "+-------+----------------+------------------+--------------------------+------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, window, sum, max, first\n",
    "from pyspark.sql.types import FloatType, TimestampType\n",
    "\n",
    "# Step 1: Create SparkSession\n",
    "spark = SparkSession.builder.appName(\"intense_window\").getOrCreate()\n",
    "\n",
    "# Step 2: Load DataFrame from CSV\n",
    "# Replace 'your_file_path.csv' with the actual path to your CSV file\n",
    "df = spark.read.csv(\"C:/Users/RaymondCarpenter/Documents/GitHub/14thstreetanalytics/src_ftbl/output_tracking.csv\", header=True)\n",
    "\n",
    "# Step 3: Convert timestamp to seconds and cast to TimestampType\n",
    "df = df.withColumn(\"timestamp_seconds\", col(\"timestamp\").cast(FloatType()))\n",
    "df = df.withColumn(\"timestamp\", col(\"timestamp\").cast(TimestampType()))\n",
    "\n",
    "# Step 4: Load player DataFrame from CSV\n",
    "player_df = spark.read.csv(\"C:/Users/RaymondCarpenter/Documents/GitHub/14thstreetanalytics/src_ftbl/DimPlayer.csv\", header=True)\n",
    "\n",
    "# Step 5: Join the tracking data with player information\n",
    "joined_df = df.join(player_df, df[\"trackable_object\"] == player_df[\"trackable_object\"])\n",
    "\n",
    "# Step 6: Aggregate into 5-minute windows, calculating intensity as the sum of distance\n",
    "intensity_df = joined_df.groupBy(\"game_id\", \"short_name\", window(\"timestamp\", \"5 minutes\")).agg(\n",
    "    sum(\"x\").alias(\"total_x\"),\n",
    "    sum(\"y\").alias(\"total_y\"),\n",
    "    sum(\"z\").alias(\"total_z\"),\n",
    "    window(\"timestamp\", \"5 minutes\").start.alias(\"window_start\"),\n",
    "    window(\"timestamp\", \"5 minutes\").end.alias(\"window_end\")\n",
    ")\n",
    "\n",
    "# Calculate overall intensity (you may need to customize this based on your specific metrics)\n",
    "intensity_df = intensity_df.withColumn(\"intensity\", col(\"total_x\") + col(\"total_y\") + col(\"total_z\"))\n",
    "\n",
    "# Step 7: Find the 5-minute window with maximum intensity for each player, for each game\n",
    "max_intensity_per_player = intensity_df.groupBy(\"game_id\", \"short_name\").agg(\n",
    "    max(\"intensity\").alias(\"max_intensity\"),\n",
    "    first(\"window_start\").alias(\"max_intensity_window_start\"),\n",
    "    first(\"window_end\").alias(\"max_intensity_window_end\")\n",
    ")\n",
    "\n",
    "# Display the result\n",
    "print(\"Most intense 5-minute window for each player, for each game:\")\n",
    "max_intensity_per_player.show(truncate=False)\n",
    "\n",
    "# Stop SparkSession\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[WinError 10061] No connection could be made because the target machine actively refused it",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\RaymondCarpenter\\Documents\\GitHub\\14thstreetanalytics\\src_ftbl\\src_ftbl_notebook.ipynb Cell 14\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/RaymondCarpenter/Documents/GitHub/14thstreetanalytics/src_ftbl/src_ftbl_notebook.ipynb#X64sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdatetime\u001b[39;00m \u001b[39mimport\u001b[39;00m datetime, timedelta\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/RaymondCarpenter/Documents/GitHub/14thstreetanalytics/src_ftbl/src_ftbl_notebook.ipynb#X64sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# Step 1: Create SparkSession\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/RaymondCarpenter/Documents/GitHub/14thstreetanalytics/src_ftbl/src_ftbl_notebook.ipynb#X64sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m spark \u001b[39m=\u001b[39m SparkSession\u001b[39m.\u001b[39;49mbuilder\u001b[39m.\u001b[39;49mappName(\u001b[39m\"\u001b[39;49m\u001b[39mintense_window1\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39;49mgetOrCreate()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/RaymondCarpenter/Documents/GitHub/14thstreetanalytics/src_ftbl/src_ftbl_notebook.ipynb#X64sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# Step 2: Load DataFrame from CSV\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/RaymondCarpenter/Documents/GitHub/14thstreetanalytics/src_ftbl/src_ftbl_notebook.ipynb#X64sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# Replace 'your_file_path.csv' with the actual path to your CSV file\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/RaymondCarpenter/Documents/GitHub/14thstreetanalytics/src_ftbl/src_ftbl_notebook.ipynb#X64sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m df \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39mread\u001b[39m.\u001b[39mcsv(\u001b[39m\"\u001b[39m\u001b[39mC:/Users/RaymondCarpenter/Documents/GitHub/14thstreetanalytics/src_ftbl/output_tracking.csv\u001b[39m\u001b[39m\"\u001b[39m, header\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\RaymondCarpenter\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pyspark\\sql\\session.py:493\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    491\u001b[0m session \u001b[39m=\u001b[39m SparkSession\u001b[39m.\u001b[39m_instantiatedSession\n\u001b[0;32m    492\u001b[0m \u001b[39mif\u001b[39;00m session \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m session\u001b[39m.\u001b[39m_sc\u001b[39m.\u001b[39m_jsc \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 493\u001b[0m     sparkConf \u001b[39m=\u001b[39m SparkConf()\n\u001b[0;32m    494\u001b[0m     \u001b[39mfor\u001b[39;00m key, value \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_options\u001b[39m.\u001b[39mitems():\n\u001b[0;32m    495\u001b[0m         sparkConf\u001b[39m.\u001b[39mset(key, value)\n",
      "File \u001b[1;32mc:\\Users\\RaymondCarpenter\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pyspark\\conf.py:132\u001b[0m, in \u001b[0;36mSparkConf.__init__\u001b[1;34m(self, loadDefaults, _jvm, _jconf)\u001b[0m\n\u001b[0;32m    128\u001b[0m _jvm \u001b[39m=\u001b[39m _jvm \u001b[39mor\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_jvm\n\u001b[0;32m    130\u001b[0m \u001b[39mif\u001b[39;00m _jvm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    131\u001b[0m     \u001b[39m# JVM is created, so create self._jconf directly through JVM\u001b[39;00m\n\u001b[1;32m--> 132\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jconf \u001b[39m=\u001b[39m _jvm\u001b[39m.\u001b[39;49mSparkConf(loadDefaults)\n\u001b[0;32m    133\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_conf \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    134\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    135\u001b[0m     \u001b[39m# JVM is not created, so store data in self._conf first\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\RaymondCarpenter\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\py4j\\java_gateway.py:1712\u001b[0m, in \u001b[0;36mJVMView.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1709\u001b[0m \u001b[39mif\u001b[39;00m name \u001b[39m==\u001b[39m UserHelpAutoCompletion\u001b[39m.\u001b[39mKEY:\n\u001b[0;32m   1710\u001b[0m     \u001b[39mreturn\u001b[39;00m UserHelpAutoCompletion()\n\u001b[1;32m-> 1712\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_gateway_client\u001b[39m.\u001b[39;49msend_command(\n\u001b[0;32m   1713\u001b[0m     proto\u001b[39m.\u001b[39;49mREFLECTION_COMMAND_NAME \u001b[39m+\u001b[39;49m\n\u001b[0;32m   1714\u001b[0m     proto\u001b[39m.\u001b[39;49mREFL_GET_UNKNOWN_SUB_COMMAND_NAME \u001b[39m+\u001b[39;49m name \u001b[39m+\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\"\u001b[39;49m \u001b[39m+\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_id \u001b[39m+\u001b[39;49m\n\u001b[0;32m   1715\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\"\u001b[39;49m \u001b[39m+\u001b[39;49m proto\u001b[39m.\u001b[39;49mEND_COMMAND_PART)\n\u001b[0;32m   1716\u001b[0m \u001b[39mif\u001b[39;00m answer \u001b[39m==\u001b[39m proto\u001b[39m.\u001b[39mSUCCESS_PACKAGE:\n\u001b[0;32m   1717\u001b[0m     \u001b[39mreturn\u001b[39;00m JavaPackage(name, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gateway_client, jvm_id\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_id)\n",
      "File \u001b[1;32mc:\\Users\\RaymondCarpenter\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\py4j\\java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[1;34m(self, command, retry, binary)\u001b[0m\n\u001b[0;32m   1015\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msend_command\u001b[39m(\u001b[39mself\u001b[39m, command, retry\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, binary\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m   1016\u001b[0m     \u001b[39m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[0;32m   1017\u001b[0m \u001b[39m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[0;32m   1018\u001b[0m \u001b[39m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1034\u001b[0m \u001b[39m     if `binary` is `True`.\u001b[39;00m\n\u001b[0;32m   1035\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1036\u001b[0m     connection \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_connection()\n\u001b[0;32m   1037\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1038\u001b[0m         response \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39msend_command(command)\n",
      "File \u001b[1;32mc:\\Users\\RaymondCarpenter\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\py4j\\clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    281\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[0;32m    283\u001b[0m \u001b[39mif\u001b[39;00m connection \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m connection\u001b[39m.\u001b[39msocket \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 284\u001b[0m     connection \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_new_connection()\n\u001b[0;32m    285\u001b[0m \u001b[39mreturn\u001b[39;00m connection\n",
      "File \u001b[1;32mc:\\Users\\RaymondCarpenter\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\py4j\\clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_create_new_connection\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    288\u001b[0m     connection \u001b[39m=\u001b[39m ClientServerConnection(\n\u001b[0;32m    289\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjava_parameters, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpython_parameters,\n\u001b[0;32m    290\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_property, \u001b[39mself\u001b[39m)\n\u001b[1;32m--> 291\u001b[0m     connection\u001b[39m.\u001b[39;49mconnect_to_java_server()\n\u001b[0;32m    292\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_thread_connection(connection)\n\u001b[0;32m    293\u001b[0m     \u001b[39mreturn\u001b[39;00m connection\n",
      "File \u001b[1;32mc:\\Users\\RaymondCarpenter\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\py4j\\clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    435\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mssl_context:\n\u001b[0;32m    436\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msocket \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mssl_context\u001b[39m.\u001b[39mwrap_socket(\n\u001b[0;32m    437\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msocket, server_hostname\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjava_address)\n\u001b[1;32m--> 438\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msocket\u001b[39m.\u001b[39;49mconnect((\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mjava_address, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mjava_port))\n\u001b[0;32m    439\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msocket\u001b[39m.\u001b[39mmakefile(\u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    440\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_connected \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[1;31mConnectionRefusedError\u001b[0m: [WinError 10061] No connection could be made because the target machine actively refused it"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, window, sum, max, first, date_format, hour, minute\n",
    "from pyspark.sql.types import FloatType, TimestampType\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Step 1: Create SparkSession\n",
    "spark = SparkSession.builder.appName(\"intense_window1\").getOrCreate()\n",
    "\n",
    "# Step 2: Load DataFrame from CSV\n",
    "# Replace 'your_file_path.csv' with the actual path to your CSV file\n",
    "df = spark.read.csv(\"C:/Users/RaymondCarpenter/Documents/GitHub/14thstreetanalytics/src_ftbl/output_tracking.csv\", header=True)\n",
    "\n",
    "# Step 3: Convert timestamp to seconds and cast to TimestampType\n",
    "df = df.withColumn(\"timestamp_seconds\", col(\"timestamp\").cast(FloatType()))\n",
    "df = df.withColumn(\"timestamp\", col(\"timestamp\").cast(TimestampType()))\n",
    "\n",
    "# Step 4: Load player DataFrame from CSV\n",
    "player_df = spark.read.csv(\"C:/Users/RaymondCarpenter/Documents/GitHub/14thstreetanalytics/src_ftbl/DimPlayer.csv\", header=True)\n",
    "\n",
    "# Step 5: Join the tracking data with player information\n",
    "joined_df = df.join(player_df, df[\"trackable_object\"] == player_df[\"trackable_object\"])\n",
    "\n",
    "# Step 6: Aggregate into 5-minute windows, calculating intensity as the sum of distance\n",
    "intensity_df = joined_df.groupBy(\"game_id\", \"short_name\", window(\"timestamp\", \"5 minutes\")).agg(\n",
    "    sum(\"x\").alias(\"total_x\"),\n",
    "    sum(\"y\").alias(\"total_y\"),\n",
    "    sum(\"z\").alias(\"total_z\"),\n",
    "    window(\"timestamp\", \"5 minutes\").start.alias(\"window_start\"),\n",
    "    window(\"timestamp\", \"5 minutes\").end.alias(\"window_end\")\n",
    ")\n",
    "\n",
    "# Calculate overall intensity (you may need to customize this based on your specific metrics)\n",
    "intensity_df = intensity_df.withColumn(\"intensity\", col(\"total_x\") + col(\"total_y\") + col(\"total_z\"))\n",
    "\n",
    "# Step 7: Find the 5-minute window with maximum intensity for each player, for each game\n",
    "max_intensity_per_player = intensity_df.groupBy(\"game_id\", \"short_name\").agg(\n",
    "    max(\"intensity\").alias(\"max_intensity\"),\n",
    "    first(date_format(\"window_start\", \"HH:mm:ss\")).alias(\"max_intensity_window_start\"),\n",
    "    first(date_format(\"window_end\", \"HH:mm:ss\")).alias(\"max_intensity_window_end\")\n",
    ")\n",
    "\n",
    "# Calculate the game duration (adjust accordingly)\n",
    "game_duration = 92\n",
    "\n",
    "# Convert the output to soccer game minute terms\n",
    "max_intensity_per_player = max_intensity_per_player.withColumn(\n",
    "    \"max_intensity_window_start_minute\",\n",
    "    (hour(\"max_intensity_window_start\") * 60 + minute(\"max_intensity_window_start\")) % game_duration\n",
    ")\n",
    "\n",
    "max_intensity_per_player = max_intensity_per_player.withColumn(\n",
    "    \"max_intensity_window_end_minute\",\n",
    "    (hour(\"max_intensity_window_end\") * 60 + minute(\"max_intensity_window_end\")) % game_duration\n",
    ")\n",
    "\n",
    "# Display the result\n",
    "print(\"Most intense 5-minute window for each player, for each game:\")\n",
    "max_intensity_per_player.show(truncate=False)\n",
    "\n",
    "# Stop SparkSession\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
